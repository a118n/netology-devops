ipvs. Если при запросе на VIP сделать подряд несколько запросов (например, for i in {1..50}; do curl -I -s 172.28.128.200>/dev/null; done ), ответы будут получены почти мгновенно. Тем не менее, в выводе ipvsadm -Ln еще некоторое время будут висеть активные InActConn. Почему так происходит?

curl использует TCP, а как мы знаем tcp-сессии окончательно закрываются после таймаута. Скорее всего, это именно эти сессии, в которых инициатор еще висит в состоянии TIME WAIT.



На лекции мы познакомились отдельно с ipvs и отдельно с keepalived. Воспользовавшись этими знаниями, совместите технологии вместе (VIP должен подниматься демоном keepalived). Приложите конфигурационные файлы, которые у вас получились, и продемонстрируйте работу получившейся конструкции. Используйте для директора отдельный хост, не совмещая его с риалом! Подобная схема возможна, но выходит за рамки рассмотренного на лекции.

Итак, имеем 5 хостов:
192.168.1.10 - ipvs/keepalived
192.168.1.20 - ipvs/keepalived
192.168.1.30 - nginx
192.168.1.40 - nginx
192.168.1.50 - test node, не участвует в балансировке

1. Настраиваем балансировщики:
sudo apt-get install -y keepalived ipvsadm
Настраиваем ipvs:
sudoedit /etc/default/ipvsadm (конфиг прилагаю)
sudo ipvsadm -A -t 192.168.1.200:80 -s rr
sudo ipvsadm -a -t 192.168.1.200:80 -r 192.168.1.30:80 -m
sudo ipvsadm -a -t 192.168.1.200:80 -r 192.168.1.40:80 -m
sudo ipvsadm -Ln --stats (убеждаемся что все верно добавилось)
sudo ipvsadm -S | sudo tee /etc/ipvsadm.rules
Далее настроим keepalived:
sudoedit /etc/keepalived/keepalived.conf (на втором хосте делаем приоритет поменьше, например 90. в остальном конфиг идентичный)
sudo systemctl restart keepalived && systemctl status keepalived (проверяем, что на первом хосте keepalived встал в master, на втором в backup)
ip -4 a s eth1 (проверяем, что адрес забиндился на нужный нам интерфейс)
На этом настройка балансировщиков в принципе завершена, ребутаем оба хоста, чтобы проверить что все корректно поднимается после перезагрузки.

2. Настраиваем nginx-ноды:
sudo apt-get install -y nginx
sudo systemctl enable --now nginx && systemctl status nginx
sudoedit /etc/netplan/01-netcfg.yaml (добавляем адрес 192.168.1.200/32)
sudo netplan apply
ip -4 a s eth1 (проверяем что адрес добавился)

Проверяем с 5 хоста, что nginx работает:
curl -I -s 192.168.1.30
curl -I -s 192.168.1.40

3. Проверяем, что работает балансировщик:
for i in {1..50}; do curl -I -s 192.168.1.200>/dev/null; done (делаем 50 запросов, c 5 хоста)
sudo ipvsadm -Ln --stats (на балансировщике)
wc -l /var/log/nginx/access.log (на серверах с nginx)

А теперь выключим интерфейс на netology1 и проверим keepalived:
sudo ip link set eth1 down
Адрес перебрасывается на netology2 и балансировщик продолжает работать.



В лекции мы использовали только 1 VIP адрес для балансировки. У такого подхода несколько отрицательных моментов, один из которых – невозможность активного использования нескольких хостов (1 адрес может только переехать с master на standby). Подумайте, сколько адресов оптимально использовать, если мы хотим без какой-либо деградации выдерживать потерю 1 из 3 хостов при входящем трафике 1.5 Гбит/с и физических линках хостов в 1 Гбит/с? Предполагается, что мы хотим задействовать 3 балансировщика в активном режиме (то есть не 2 адреса на 3 хоста, один из которых в обычное время простаивает).

Для более точечного размазывания трафика при выпадении одного хоста можно использовать шесть IP адресов в All-Active конфигурации и DNS round-robin. Тогда при выпадении хоста трафик будет равномерно распределен на оставшиеся два по 0.75 гбит\с.
